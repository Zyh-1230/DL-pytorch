{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a131a7f9-778e-42d6-9fb0-738eb1da6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2910d392-e89a-4f53-b574-66ad65ec283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "NUM_EPOCH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ec60673-dfb3-4831-acac-ad3e5da3d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OttoDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True, feature_means=None, feature_stds=None):\n",
    "        self.is_train_set = is_train_set\n",
    "        filepath = 'otto_train.csv.zip' if is_train_set else 'otto_test.csv.zip'\n",
    "        self.df = pd.read_csv(filepath, compression='zip')\n",
    "\n",
    "        if not is_train_set:\n",
    "            self.id = self.df['id'].copy()\n",
    "\n",
    "\n",
    "        self.feature_means = feature_means\n",
    "        self.feature_stds = feature_stds\n",
    "        \n",
    "        self._preprocess()\n",
    "\n",
    "        features_df = self.df.drop('target', axis=1) if 'target' in self.df.columns else self.df\n",
    "        self.features = torch.tensor(features_df.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.df['target'].values, dtype=torch.long) if 'target' in self.df.columns else torch.zeros(len(self.df), dtype=torch.long)\n",
    "        self.len = len(self.df)\n",
    "\n",
    "    def _preprocess(self):\n",
    "        self.df = self.df.drop('id', axis=1)\n",
    "        if 'target' not in self.df.columns:\n",
    "            self.df['target'] = -1\n",
    "        if self.is_train_set and 'target' in self.df.columns:\n",
    "            self.df['target'] = self.df['target'].map({'Class_1': 0, \n",
    "                                                       'Class_2': 1, \n",
    "                                                       'Class_3': 2, \n",
    "                                                       'Class_4': 3, \n",
    "                                                       'Class_5': 4, \n",
    "                                                       'Class_6': 5, \n",
    "                                                       'Class_7': 6, \n",
    "                                                       'Class_8': 7, \n",
    "                                                       'Class_9': 8})\n",
    "\n",
    "        if 'target' in self.df.columns:\n",
    "            features = self.df.drop('target', axis=1)\n",
    "            targets = self.df['target']\n",
    "        else:\n",
    "            features = self.df.copy()\n",
    "            targets = None\n",
    "\n",
    "        if self.is_train_set:\n",
    "            self.feature_means = features.mean(axis=0)\n",
    "            self.feature_stds = features.std(axis=0)\n",
    "            self.feature_stds = self.feature_stds.replace(0, 1)\n",
    "\n",
    "        elif self.feature_means is None or self.feature_stds is None:\n",
    "            raise ValueError(\"Test set requires precomputed feature means/stds\")\n",
    "\n",
    "        features = (features - self.feature_means) / (self.feature_stds + 1e-8)\n",
    "\n",
    "        if targets is not None:\n",
    "            self.df = pd.concat([features, targets], axis=1)\n",
    "        else:\n",
    "            self.df = features\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb997596-5780-4cd0-a4e2-0e3431f28293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = OttoDataset(is_train_set=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_set = OttoDataset(\n",
    "    is_train_set=False,\n",
    "    feature_means=train_set.feature_means,\n",
    "    feature_stds=train_set.feature_stds\n",
    ")\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e48b7559-5110-4b95-bb4c-f09d4f5d6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(93, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 128)\n",
    "        self.fc4 = torch.nn.Linear(128, 64)\n",
    "        self.fc5 = torch.nn.Linear(64, 32)\n",
    "        self.fc6 = torch.nn.Linear(32, 9)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 93)\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40377eea-4b95-425c-82e7-affe7bec597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "\n",
    "best_accuracy = 0\n",
    "no_improve_count = 0\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3409df75-0704-458e-b073-87ef7c8aad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [100/484], Loss: 1.3803, LR: 0.001000\n",
      "Epoch [1], Batch [200/484], Loss: 0.9031, LR: 0.001000\n",
      "Epoch [1], Batch [300/484], Loss: 0.7976, LR: 0.001000\n",
      "Epoch [1], Batch [400/484], Loss: 0.7638, LR: 0.001000\n",
      "Evaluation Loss: 0.6269, Accuracy: 75.31%\n",
      "New best model saved with accuracy: 75.31%\n",
      "Epoch [2], Batch [100/484], Loss: 0.7031, LR: 0.001000\n",
      "Epoch [2], Batch [200/484], Loss: 0.6996, LR: 0.001000\n",
      "Epoch [2], Batch [300/484], Loss: 0.6785, LR: 0.001000\n",
      "Epoch [2], Batch [400/484], Loss: 0.6876, LR: 0.001000\n",
      "Evaluation Loss: 0.5749, Accuracy: 77.33%\n",
      "New best model saved with accuracy: 77.33%\n",
      "Epoch [3], Batch [100/484], Loss: 0.6429, LR: 0.001000\n",
      "Epoch [3], Batch [200/484], Loss: 0.6598, LR: 0.001000\n",
      "Epoch [3], Batch [300/484], Loss: 0.6521, LR: 0.001000\n",
      "Epoch [3], Batch [400/484], Loss: 0.6471, LR: 0.001000\n",
      "Evaluation Loss: 0.5557, Accuracy: 78.44%\n",
      "New best model saved with accuracy: 78.44%\n",
      "Epoch [4], Batch [100/484], Loss: 0.6533, LR: 0.001000\n",
      "Epoch [4], Batch [200/484], Loss: 0.6260, LR: 0.001000\n",
      "Epoch [4], Batch [300/484], Loss: 0.6356, LR: 0.001000\n",
      "Epoch [4], Batch [400/484], Loss: 0.6309, LR: 0.001000\n",
      "Evaluation Loss: 0.5396, Accuracy: 78.98%\n",
      "New best model saved with accuracy: 78.98%\n",
      "Epoch [5], Batch [100/484], Loss: 0.6196, LR: 0.001000\n",
      "Epoch [5], Batch [200/484], Loss: 0.6255, LR: 0.001000\n",
      "Epoch [5], Batch [300/484], Loss: 0.6284, LR: 0.001000\n",
      "Epoch [5], Batch [400/484], Loss: 0.6090, LR: 0.001000\n",
      "Evaluation Loss: 0.5224, Accuracy: 79.54%\n",
      "New best model saved with accuracy: 79.54%\n",
      "Epoch [6], Batch [100/484], Loss: 0.5944, LR: 0.001000\n",
      "Epoch [6], Batch [200/484], Loss: 0.6099, LR: 0.001000\n",
      "Epoch [6], Batch [300/484], Loss: 0.6122, LR: 0.001000\n",
      "Epoch [6], Batch [400/484], Loss: 0.6026, LR: 0.001000\n",
      "Evaluation Loss: 0.5114, Accuracy: 79.97%\n",
      "New best model saved with accuracy: 79.97%\n",
      "Epoch [7], Batch [100/484], Loss: 0.5953, LR: 0.001000\n",
      "Epoch [7], Batch [200/484], Loss: 0.5912, LR: 0.001000\n",
      "Epoch [7], Batch [300/484], Loss: 0.6021, LR: 0.001000\n",
      "Epoch [7], Batch [400/484], Loss: 0.5957, LR: 0.001000\n",
      "Evaluation Loss: 0.5084, Accuracy: 80.15%\n",
      "New best model saved with accuracy: 80.15%\n",
      "Epoch [8], Batch [100/484], Loss: 0.5950, LR: 0.001000\n",
      "Epoch [8], Batch [200/484], Loss: 0.5959, LR: 0.001000\n",
      "Epoch [8], Batch [300/484], Loss: 0.5827, LR: 0.001000\n",
      "Epoch [8], Batch [400/484], Loss: 0.5821, LR: 0.001000\n",
      "Evaluation Loss: 0.4919, Accuracy: 80.41%\n",
      "New best model saved with accuracy: 80.41%\n",
      "Epoch [9], Batch [100/484], Loss: 0.5717, LR: 0.001000\n",
      "Epoch [9], Batch [200/484], Loss: 0.5764, LR: 0.001000\n",
      "Epoch [9], Batch [300/484], Loss: 0.5692, LR: 0.001000\n",
      "Epoch [9], Batch [400/484], Loss: 0.5810, LR: 0.001000\n",
      "Evaluation Loss: 0.4880, Accuracy: 80.82%\n",
      "New best model saved with accuracy: 80.82%\n",
      "Epoch [10], Batch [100/484], Loss: 0.5552, LR: 0.001000\n",
      "Epoch [10], Batch [200/484], Loss: 0.5705, LR: 0.001000\n",
      "Epoch [10], Batch [300/484], Loss: 0.5707, LR: 0.001000\n",
      "Epoch [10], Batch [400/484], Loss: 0.5690, LR: 0.001000\n",
      "Evaluation Loss: 0.4827, Accuracy: 80.91%\n",
      "New best model saved with accuracy: 80.91%\n",
      "Epoch [10/300], Train Loss: 0.1028, Acc:  78.27%\n",
      "Epoch [11], Batch [100/484], Loss: 0.5519, LR: 0.001000\n",
      "Epoch [11], Batch [200/484], Loss: 0.5689, LR: 0.001000\n",
      "Epoch [11], Batch [300/484], Loss: 0.5577, LR: 0.001000\n",
      "Epoch [11], Batch [400/484], Loss: 0.5563, LR: 0.001000\n",
      "Evaluation Loss: 0.4778, Accuracy: 80.58%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [12], Batch [100/484], Loss: 0.5473, LR: 0.001000\n",
      "Epoch [12], Batch [200/484], Loss: 0.5804, LR: 0.001000\n",
      "Epoch [12], Batch [300/484], Loss: 0.5593, LR: 0.001000\n",
      "Epoch [12], Batch [400/484], Loss: 0.5499, LR: 0.001000\n",
      "Evaluation Loss: 0.4715, Accuracy: 81.47%\n",
      "New best model saved with accuracy: 81.47%\n",
      "Epoch [13], Batch [100/484], Loss: 0.5389, LR: 0.001000\n",
      "Epoch [13], Batch [200/484], Loss: 0.5466, LR: 0.001000\n",
      "Epoch [13], Batch [300/484], Loss: 0.5552, LR: 0.001000\n",
      "Epoch [13], Batch [400/484], Loss: 0.5660, LR: 0.001000\n",
      "Evaluation Loss: 0.4668, Accuracy: 81.34%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [14], Batch [100/484], Loss: 0.5397, LR: 0.001000\n",
      "Epoch [14], Batch [200/484], Loss: 0.5399, LR: 0.001000\n",
      "Epoch [14], Batch [300/484], Loss: 0.5553, LR: 0.001000\n",
      "Epoch [14], Batch [400/484], Loss: 0.5633, LR: 0.001000\n",
      "Evaluation Loss: 0.4664, Accuracy: 81.82%\n",
      "New best model saved with accuracy: 81.82%\n",
      "Epoch [15], Batch [100/484], Loss: 0.5533, LR: 0.001000\n",
      "Epoch [15], Batch [200/484], Loss: 0.5327, LR: 0.001000\n",
      "Epoch [15], Batch [300/484], Loss: 0.5458, LR: 0.001000\n",
      "Epoch [15], Batch [400/484], Loss: 0.5463, LR: 0.001000\n",
      "Evaluation Loss: 0.4567, Accuracy: 81.85%\n",
      "New best model saved with accuracy: 81.85%\n",
      "Epoch [16], Batch [100/484], Loss: 0.5403, LR: 0.001000\n",
      "Epoch [16], Batch [200/484], Loss: 0.5322, LR: 0.001000\n",
      "Epoch [16], Batch [300/484], Loss: 0.5552, LR: 0.001000\n",
      "Epoch [16], Batch [400/484], Loss: 0.5383, LR: 0.001000\n",
      "Evaluation Loss: 0.4543, Accuracy: 82.06%\n",
      "New best model saved with accuracy: 82.06%\n",
      "Epoch [17], Batch [100/484], Loss: 0.5406, LR: 0.001000\n",
      "Epoch [17], Batch [200/484], Loss: 0.5331, LR: 0.001000\n",
      "Epoch [17], Batch [300/484], Loss: 0.5380, LR: 0.001000\n",
      "Epoch [17], Batch [400/484], Loss: 0.5370, LR: 0.001000\n",
      "Evaluation Loss: 0.4543, Accuracy: 81.95%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [18], Batch [100/484], Loss: 0.5162, LR: 0.001000\n",
      "Epoch [18], Batch [200/484], Loss: 0.5394, LR: 0.001000\n",
      "Epoch [18], Batch [300/484], Loss: 0.5355, LR: 0.001000\n",
      "Epoch [18], Batch [400/484], Loss: 0.5364, LR: 0.001000\n",
      "Evaluation Loss: 0.4439, Accuracy: 82.24%\n",
      "New best model saved with accuracy: 82.24%\n",
      "Epoch [19], Batch [100/484], Loss: 0.5132, LR: 0.001000\n",
      "Epoch [19], Batch [200/484], Loss: 0.5284, LR: 0.001000\n",
      "Epoch [19], Batch [300/484], Loss: 0.5480, LR: 0.001000\n",
      "Epoch [19], Batch [400/484], Loss: 0.5442, LR: 0.001000\n",
      "Evaluation Loss: 0.4404, Accuracy: 82.58%\n",
      "New best model saved with accuracy: 82.58%\n",
      "Epoch [20], Batch [100/484], Loss: 0.5258, LR: 0.001000\n",
      "Epoch [20], Batch [200/484], Loss: 0.5197, LR: 0.001000\n",
      "Epoch [20], Batch [300/484], Loss: 0.5261, LR: 0.001000\n",
      "Epoch [20], Batch [400/484], Loss: 0.5399, LR: 0.001000\n",
      "Evaluation Loss: 0.4376, Accuracy: 82.59%\n",
      "New best model saved with accuracy: 82.59%\n",
      "Epoch [20/300], Train Loss: 0.0911, Acc:  78.27%\n",
      "Epoch [21], Batch [100/484], Loss: 0.5207, LR: 0.001000\n",
      "Epoch [21], Batch [200/484], Loss: 0.5122, LR: 0.001000\n",
      "Epoch [21], Batch [300/484], Loss: 0.5417, LR: 0.001000\n",
      "Epoch [21], Batch [400/484], Loss: 0.5221, LR: 0.001000\n",
      "Evaluation Loss: 0.4314, Accuracy: 82.60%\n",
      "New best model saved with accuracy: 82.60%\n",
      "Epoch [22], Batch [100/484], Loss: 0.5202, LR: 0.001000\n",
      "Epoch [22], Batch [200/484], Loss: 0.5286, LR: 0.001000\n",
      "Epoch [22], Batch [300/484], Loss: 0.5185, LR: 0.001000\n",
      "Epoch [22], Batch [400/484], Loss: 0.5221, LR: 0.001000\n",
      "Evaluation Loss: 0.4267, Accuracy: 83.06%\n",
      "New best model saved with accuracy: 83.06%\n",
      "Epoch [23], Batch [100/484], Loss: 0.5144, LR: 0.001000\n",
      "Epoch [23], Batch [200/484], Loss: 0.5218, LR: 0.001000\n",
      "Epoch [23], Batch [300/484], Loss: 0.5205, LR: 0.001000\n",
      "Epoch [23], Batch [400/484], Loss: 0.5202, LR: 0.001000\n",
      "Evaluation Loss: 0.4257, Accuracy: 83.15%\n",
      "New best model saved with accuracy: 83.15%\n",
      "Epoch [24], Batch [100/484], Loss: 0.5195, LR: 0.001000\n",
      "Epoch [24], Batch [200/484], Loss: 0.5146, LR: 0.001000\n",
      "Epoch [24], Batch [300/484], Loss: 0.5173, LR: 0.001000\n",
      "Epoch [24], Batch [400/484], Loss: 0.5148, LR: 0.001000\n",
      "Evaluation Loss: 0.4265, Accuracy: 82.97%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [25], Batch [100/484], Loss: 0.5096, LR: 0.001000\n",
      "Epoch [25], Batch [200/484], Loss: 0.5281, LR: 0.001000\n",
      "Epoch [25], Batch [300/484], Loss: 0.5047, LR: 0.001000\n",
      "Epoch [25], Batch [400/484], Loss: 0.5225, LR: 0.001000\n",
      "Evaluation Loss: 0.4263, Accuracy: 82.98%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [26], Batch [100/484], Loss: 0.4981, LR: 0.001000\n",
      "Epoch [26], Batch [200/484], Loss: 0.5120, LR: 0.001000\n",
      "Epoch [26], Batch [300/484], Loss: 0.5215, LR: 0.001000\n",
      "Epoch [26], Batch [400/484], Loss: 0.5131, LR: 0.001000\n",
      "Evaluation Loss: 0.4219, Accuracy: 83.23%\n",
      "New best model saved with accuracy: 83.23%\n",
      "Epoch [27], Batch [100/484], Loss: 0.5053, LR: 0.001000\n",
      "Epoch [27], Batch [200/484], Loss: 0.5026, LR: 0.001000\n",
      "Epoch [27], Batch [300/484], Loss: 0.5036, LR: 0.001000\n",
      "Epoch [27], Batch [400/484], Loss: 0.5179, LR: 0.001000\n",
      "Evaluation Loss: 0.4204, Accuracy: 83.12%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [28], Batch [100/484], Loss: 0.4989, LR: 0.001000\n",
      "Epoch [28], Batch [200/484], Loss: 0.5148, LR: 0.001000\n",
      "Epoch [28], Batch [300/484], Loss: 0.5110, LR: 0.001000\n",
      "Epoch [28], Batch [400/484], Loss: 0.5020, LR: 0.001000\n",
      "Evaluation Loss: 0.4177, Accuracy: 83.20%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [29], Batch [100/484], Loss: 0.5076, LR: 0.001000\n",
      "Epoch [29], Batch [200/484], Loss: 0.5013, LR: 0.001000\n",
      "Epoch [29], Batch [300/484], Loss: 0.4967, LR: 0.001000\n",
      "Epoch [29], Batch [400/484], Loss: 0.5058, LR: 0.001000\n",
      "Evaluation Loss: 0.4134, Accuracy: 83.38%\n",
      "New best model saved with accuracy: 83.38%\n",
      "Epoch [30], Batch [100/484], Loss: 0.5003, LR: 0.001000\n",
      "Epoch [30], Batch [200/484], Loss: 0.4981, LR: 0.001000\n",
      "Epoch [30], Batch [300/484], Loss: 0.5210, LR: 0.001000\n",
      "Epoch [30], Batch [400/484], Loss: 0.5145, LR: 0.001000\n",
      "Evaluation Loss: 0.4142, Accuracy: 83.26%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [30/300], Train Loss: 0.0871, Acc:  78.27%\n",
      "Epoch [31], Batch [100/484], Loss: 0.4962, LR: 0.001000\n",
      "Epoch [31], Batch [200/484], Loss: 0.4927, LR: 0.001000\n",
      "Epoch [31], Batch [300/484], Loss: 0.5034, LR: 0.001000\n",
      "Epoch [31], Batch [400/484], Loss: 0.5114, LR: 0.001000\n",
      "Evaluation Loss: 0.4086, Accuracy: 83.67%\n",
      "New best model saved with accuracy: 83.67%\n",
      "Epoch [32], Batch [100/484], Loss: 0.5032, LR: 0.001000\n",
      "Epoch [32], Batch [200/484], Loss: 0.5036, LR: 0.001000\n",
      "Epoch [32], Batch [300/484], Loss: 0.5053, LR: 0.001000\n",
      "Epoch [32], Batch [400/484], Loss: 0.4904, LR: 0.001000\n",
      "Evaluation Loss: 0.4078, Accuracy: 83.79%\n",
      "New best model saved with accuracy: 83.79%\n",
      "Epoch [33], Batch [100/484], Loss: 0.4819, LR: 0.001000\n",
      "Epoch [33], Batch [200/484], Loss: 0.4918, LR: 0.001000\n",
      "Epoch [33], Batch [300/484], Loss: 0.5036, LR: 0.001000\n",
      "Epoch [33], Batch [400/484], Loss: 0.5035, LR: 0.001000\n",
      "Evaluation Loss: 0.4090, Accuracy: 83.64%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [34], Batch [100/484], Loss: 0.4799, LR: 0.001000\n",
      "Epoch [34], Batch [200/484], Loss: 0.4957, LR: 0.001000\n",
      "Epoch [34], Batch [300/484], Loss: 0.5024, LR: 0.001000\n",
      "Epoch [34], Batch [400/484], Loss: 0.4969, LR: 0.001000\n",
      "Evaluation Loss: 0.4035, Accuracy: 83.89%\n",
      "New best model saved with accuracy: 83.89%\n",
      "Epoch [35], Batch [100/484], Loss: 0.4877, LR: 0.001000\n",
      "Epoch [35], Batch [200/484], Loss: 0.4945, LR: 0.001000\n",
      "Epoch [35], Batch [300/484], Loss: 0.5136, LR: 0.001000\n",
      "Epoch [35], Batch [400/484], Loss: 0.4972, LR: 0.001000\n",
      "Evaluation Loss: 0.4021, Accuracy: 83.89%\n",
      "New best model saved with accuracy: 83.89%\n",
      "Epoch [36], Batch [100/484], Loss: 0.4858, LR: 0.001000\n",
      "Epoch [36], Batch [200/484], Loss: 0.4946, LR: 0.001000\n",
      "Epoch [36], Batch [300/484], Loss: 0.4952, LR: 0.001000\n",
      "Epoch [36], Batch [400/484], Loss: 0.4929, LR: 0.001000\n",
      "Evaluation Loss: 0.4029, Accuracy: 83.90%\n",
      "New best model saved with accuracy: 83.90%\n",
      "Epoch [37], Batch [100/484], Loss: 0.4962, LR: 0.001000\n",
      "Epoch [37], Batch [200/484], Loss: 0.4956, LR: 0.001000\n",
      "Epoch [37], Batch [300/484], Loss: 0.4806, LR: 0.001000\n",
      "Epoch [37], Batch [400/484], Loss: 0.5004, LR: 0.001000\n",
      "Evaluation Loss: 0.3983, Accuracy: 84.07%\n",
      "New best model saved with accuracy: 84.07%\n",
      "Epoch [38], Batch [100/484], Loss: 0.4867, LR: 0.001000\n",
      "Epoch [38], Batch [200/484], Loss: 0.4817, LR: 0.001000\n",
      "Epoch [38], Batch [300/484], Loss: 0.4841, LR: 0.001000\n",
      "Epoch [38], Batch [400/484], Loss: 0.4929, LR: 0.001000\n",
      "Evaluation Loss: 0.3964, Accuracy: 84.14%\n",
      "New best model saved with accuracy: 84.14%\n",
      "Epoch [39], Batch [100/484], Loss: 0.4996, LR: 0.001000\n",
      "Epoch [39], Batch [200/484], Loss: 0.4848, LR: 0.001000\n",
      "Epoch [39], Batch [300/484], Loss: 0.4825, LR: 0.001000\n",
      "Epoch [39], Batch [400/484], Loss: 0.4857, LR: 0.001000\n",
      "Evaluation Loss: 0.3960, Accuracy: 84.13%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [40], Batch [100/484], Loss: 0.4858, LR: 0.001000\n",
      "Epoch [40], Batch [200/484], Loss: 0.4867, LR: 0.001000\n",
      "Epoch [40], Batch [300/484], Loss: 0.4700, LR: 0.001000\n",
      "Epoch [40], Batch [400/484], Loss: 0.4899, LR: 0.001000\n",
      "Evaluation Loss: 0.3942, Accuracy: 84.12%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [40/300], Train Loss: 0.0886, Acc:  78.27%\n",
      "Epoch [41], Batch [100/484], Loss: 0.4917, LR: 0.001000\n",
      "Epoch [41], Batch [200/484], Loss: 0.4947, LR: 0.001000\n",
      "Epoch [41], Batch [300/484], Loss: 0.4809, LR: 0.001000\n",
      "Epoch [41], Batch [400/484], Loss: 0.4988, LR: 0.001000\n",
      "Evaluation Loss: 0.3895, Accuracy: 84.34%\n",
      "New best model saved with accuracy: 84.34%\n",
      "Epoch [42], Batch [100/484], Loss: 0.4752, LR: 0.001000\n",
      "Epoch [42], Batch [200/484], Loss: 0.4940, LR: 0.001000\n",
      "Epoch [42], Batch [300/484], Loss: 0.4843, LR: 0.001000\n",
      "Epoch [42], Batch [400/484], Loss: 0.4952, LR: 0.001000\n",
      "Evaluation Loss: 0.3902, Accuracy: 84.30%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [43], Batch [100/484], Loss: 0.4801, LR: 0.001000\n",
      "Epoch [43], Batch [200/484], Loss: 0.4899, LR: 0.001000\n",
      "Epoch [43], Batch [300/484], Loss: 0.4859, LR: 0.001000\n",
      "Epoch [43], Batch [400/484], Loss: 0.4827, LR: 0.001000\n",
      "Evaluation Loss: 0.3920, Accuracy: 84.32%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [44], Batch [100/484], Loss: 0.4743, LR: 0.001000\n",
      "Epoch [44], Batch [200/484], Loss: 0.4871, LR: 0.001000\n",
      "Epoch [44], Batch [300/484], Loss: 0.4954, LR: 0.001000\n",
      "Epoch [44], Batch [400/484], Loss: 0.4889, LR: 0.001000\n",
      "Evaluation Loss: 0.3897, Accuracy: 84.25%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [45], Batch [100/484], Loss: 0.4770, LR: 0.001000\n",
      "Epoch [45], Batch [200/484], Loss: 0.4815, LR: 0.001000\n",
      "Epoch [45], Batch [300/484], Loss: 0.4784, LR: 0.001000\n",
      "Epoch [45], Batch [400/484], Loss: 0.4845, LR: 0.001000\n",
      "Evaluation Loss: 0.3866, Accuracy: 84.49%\n",
      "New best model saved with accuracy: 84.49%\n",
      "Epoch [46], Batch [100/484], Loss: 0.4780, LR: 0.001000\n",
      "Epoch [46], Batch [200/484], Loss: 0.4899, LR: 0.001000\n",
      "Epoch [46], Batch [300/484], Loss: 0.4858, LR: 0.001000\n",
      "Epoch [46], Batch [400/484], Loss: 0.4858, LR: 0.001000\n",
      "Evaluation Loss: 0.3871, Accuracy: 84.40%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [47], Batch [100/484], Loss: 0.4797, LR: 0.001000\n",
      "Epoch [47], Batch [200/484], Loss: 0.4752, LR: 0.001000\n",
      "Epoch [47], Batch [300/484], Loss: 0.4755, LR: 0.001000\n",
      "Epoch [47], Batch [400/484], Loss: 0.4940, LR: 0.001000\n",
      "Evaluation Loss: 0.3848, Accuracy: 84.62%\n",
      "New best model saved with accuracy: 84.62%\n",
      "Epoch [48], Batch [100/484], Loss: 0.4709, LR: 0.001000\n",
      "Epoch [48], Batch [200/484], Loss: 0.4801, LR: 0.001000\n",
      "Epoch [48], Batch [300/484], Loss: 0.4705, LR: 0.001000\n",
      "Epoch [48], Batch [400/484], Loss: 0.4866, LR: 0.001000\n",
      "Evaluation Loss: 0.3856, Accuracy: 84.65%\n",
      "New best model saved with accuracy: 84.65%\n",
      "Epoch [49], Batch [100/484], Loss: 0.4705, LR: 0.001000\n",
      "Epoch [49], Batch [200/484], Loss: 0.4867, LR: 0.001000\n",
      "Epoch [49], Batch [300/484], Loss: 0.4853, LR: 0.001000\n",
      "Epoch [49], Batch [400/484], Loss: 0.4773, LR: 0.001000\n",
      "Evaluation Loss: 0.3840, Accuracy: 84.60%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [50], Batch [100/484], Loss: 0.4826, LR: 0.001000\n",
      "Epoch [50], Batch [200/484], Loss: 0.4753, LR: 0.001000\n",
      "Epoch [50], Batch [300/484], Loss: 0.4723, LR: 0.001000\n",
      "Epoch [50], Batch [400/484], Loss: 0.4841, LR: 0.001000\n",
      "Evaluation Loss: 0.3857, Accuracy: 84.24%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [50/300], Train Loss: 0.0834, Acc:  78.27%\n",
      "Epoch [51], Batch [100/484], Loss: 0.4776, LR: 0.001000\n",
      "Epoch [51], Batch [200/484], Loss: 0.4760, LR: 0.001000\n",
      "Epoch [51], Batch [300/484], Loss: 0.4782, LR: 0.001000\n",
      "Epoch [51], Batch [400/484], Loss: 0.4823, LR: 0.001000\n",
      "Evaluation Loss: 0.3808, Accuracy: 84.83%\n",
      "New best model saved with accuracy: 84.83%\n",
      "Epoch [52], Batch [100/484], Loss: 0.4730, LR: 0.001000\n",
      "Epoch [52], Batch [200/484], Loss: 0.4891, LR: 0.001000\n",
      "Epoch [52], Batch [300/484], Loss: 0.4878, LR: 0.001000\n",
      "Epoch [52], Batch [400/484], Loss: 0.4672, LR: 0.001000\n",
      "Evaluation Loss: 0.3789, Accuracy: 84.72%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [53], Batch [100/484], Loss: 0.4735, LR: 0.001000\n",
      "Epoch [53], Batch [200/484], Loss: 0.4593, LR: 0.001000\n",
      "Epoch [53], Batch [300/484], Loss: 0.4771, LR: 0.001000\n",
      "Epoch [53], Batch [400/484], Loss: 0.4849, LR: 0.001000\n",
      "Evaluation Loss: 0.3821, Accuracy: 84.71%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [54], Batch [100/484], Loss: 0.4646, LR: 0.001000\n",
      "Epoch [54], Batch [200/484], Loss: 0.4834, LR: 0.001000\n",
      "Epoch [54], Batch [300/484], Loss: 0.4688, LR: 0.001000\n",
      "Epoch [54], Batch [400/484], Loss: 0.4812, LR: 0.001000\n",
      "Evaluation Loss: 0.3789, Accuracy: 84.75%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [55], Batch [100/484], Loss: 0.4736, LR: 0.001000\n",
      "Epoch [55], Batch [200/484], Loss: 0.4699, LR: 0.001000\n",
      "Epoch [55], Batch [300/484], Loss: 0.4648, LR: 0.001000\n",
      "Epoch [55], Batch [400/484], Loss: 0.4777, LR: 0.001000\n",
      "Evaluation Loss: 0.3809, Accuracy: 84.63%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [56], Batch [100/484], Loss: 0.4688, LR: 0.001000\n",
      "Epoch [56], Batch [200/484], Loss: 0.4811, LR: 0.001000\n",
      "Epoch [56], Batch [300/484], Loss: 0.4696, LR: 0.001000\n",
      "Epoch [56], Batch [400/484], Loss: 0.4761, LR: 0.001000\n",
      "Evaluation Loss: 0.3842, Accuracy: 84.54%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [57], Batch [100/484], Loss: 0.4692, LR: 0.001000\n",
      "Epoch [57], Batch [200/484], Loss: 0.4692, LR: 0.001000\n",
      "Epoch [57], Batch [300/484], Loss: 0.4750, LR: 0.001000\n",
      "Epoch [57], Batch [400/484], Loss: 0.4756, LR: 0.001000\n",
      "Evaluation Loss: 0.3698, Accuracy: 85.05%\n",
      "New best model saved with accuracy: 85.05%\n",
      "Epoch [58], Batch [100/484], Loss: 0.4764, LR: 0.001000\n",
      "Epoch [58], Batch [200/484], Loss: 0.4733, LR: 0.001000\n",
      "Epoch [58], Batch [300/484], Loss: 0.4519, LR: 0.001000\n",
      "Epoch [58], Batch [400/484], Loss: 0.4823, LR: 0.001000\n",
      "Evaluation Loss: 0.3706, Accuracy: 85.12%\n",
      "New best model saved with accuracy: 85.12%\n",
      "Epoch [59], Batch [100/484], Loss: 0.4601, LR: 0.001000\n",
      "Epoch [59], Batch [200/484], Loss: 0.4699, LR: 0.001000\n",
      "Epoch [59], Batch [300/484], Loss: 0.4806, LR: 0.001000\n",
      "Epoch [59], Batch [400/484], Loss: 0.4718, LR: 0.001000\n",
      "Evaluation Loss: 0.3724, Accuracy: 84.97%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [60], Batch [100/484], Loss: 0.4644, LR: 0.001000\n",
      "Epoch [60], Batch [200/484], Loss: 0.4659, LR: 0.001000\n",
      "Epoch [60], Batch [300/484], Loss: 0.4845, LR: 0.001000\n",
      "Epoch [60], Batch [400/484], Loss: 0.4672, LR: 0.001000\n",
      "Evaluation Loss: 0.3709, Accuracy: 85.00%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [60/300], Train Loss: 0.0809, Acc:  78.27%\n",
      "Epoch [61], Batch [100/484], Loss: 0.4615, LR: 0.001000\n",
      "Epoch [61], Batch [200/484], Loss: 0.4688, LR: 0.001000\n",
      "Epoch [61], Batch [300/484], Loss: 0.4630, LR: 0.001000\n",
      "Epoch [61], Batch [400/484], Loss: 0.4715, LR: 0.001000\n",
      "Evaluation Loss: 0.3698, Accuracy: 85.25%\n",
      "New best model saved with accuracy: 85.25%\n",
      "Epoch [62], Batch [100/484], Loss: 0.4486, LR: 0.001000\n",
      "Epoch [62], Batch [200/484], Loss: 0.4652, LR: 0.001000\n",
      "Epoch [62], Batch [300/484], Loss: 0.4807, LR: 0.001000\n",
      "Epoch [62], Batch [400/484], Loss: 0.4631, LR: 0.001000\n",
      "Evaluation Loss: 0.3722, Accuracy: 85.05%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [63], Batch [100/484], Loss: 0.4643, LR: 0.001000\n",
      "Epoch [63], Batch [200/484], Loss: 0.4732, LR: 0.001000\n",
      "Epoch [63], Batch [300/484], Loss: 0.4617, LR: 0.001000\n",
      "Epoch [63], Batch [400/484], Loss: 0.4666, LR: 0.001000\n",
      "Evaluation Loss: 0.3689, Accuracy: 85.19%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [64], Batch [100/484], Loss: 0.4634, LR: 0.001000\n",
      "Epoch [64], Batch [200/484], Loss: 0.4731, LR: 0.001000\n",
      "Epoch [64], Batch [300/484], Loss: 0.4707, LR: 0.001000\n",
      "Epoch [64], Batch [400/484], Loss: 0.4692, LR: 0.001000\n",
      "Evaluation Loss: 0.3672, Accuracy: 85.36%\n",
      "New best model saved with accuracy: 85.36%\n",
      "Epoch [65], Batch [100/484], Loss: 0.4547, LR: 0.001000\n",
      "Epoch [65], Batch [200/484], Loss: 0.4608, LR: 0.001000\n",
      "Epoch [65], Batch [300/484], Loss: 0.4641, LR: 0.001000\n",
      "Epoch [65], Batch [400/484], Loss: 0.4732, LR: 0.001000\n",
      "Evaluation Loss: 0.3672, Accuracy: 85.28%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [66], Batch [100/484], Loss: 0.4632, LR: 0.001000\n",
      "Epoch [66], Batch [200/484], Loss: 0.4526, LR: 0.001000\n",
      "Epoch [66], Batch [300/484], Loss: 0.4742, LR: 0.001000\n",
      "Epoch [66], Batch [400/484], Loss: 0.4774, LR: 0.001000\n",
      "Evaluation Loss: 0.3709, Accuracy: 85.03%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [67], Batch [100/484], Loss: 0.4493, LR: 0.001000\n",
      "Epoch [67], Batch [200/484], Loss: 0.4613, LR: 0.001000\n",
      "Epoch [67], Batch [300/484], Loss: 0.4765, LR: 0.001000\n",
      "Epoch [67], Batch [400/484], Loss: 0.4749, LR: 0.001000\n",
      "Evaluation Loss: 0.3635, Accuracy: 85.24%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [68], Batch [100/484], Loss: 0.4668, LR: 0.001000\n",
      "Epoch [68], Batch [200/484], Loss: 0.4584, LR: 0.001000\n",
      "Epoch [68], Batch [300/484], Loss: 0.4778, LR: 0.001000\n",
      "Epoch [68], Batch [400/484], Loss: 0.4671, LR: 0.001000\n",
      "Evaluation Loss: 0.3620, Accuracy: 85.32%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [69], Batch [100/484], Loss: 0.4545, LR: 0.001000\n",
      "Epoch [69], Batch [200/484], Loss: 0.4606, LR: 0.001000\n",
      "Epoch [69], Batch [300/484], Loss: 0.4609, LR: 0.001000\n",
      "Epoch [69], Batch [400/484], Loss: 0.4667, LR: 0.001000\n",
      "Evaluation Loss: 0.3667, Accuracy: 85.22%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [70], Batch [100/484], Loss: 0.4503, LR: 0.001000\n",
      "Epoch [70], Batch [200/484], Loss: 0.4653, LR: 0.001000\n",
      "Epoch [70], Batch [300/484], Loss: 0.4573, LR: 0.001000\n",
      "Epoch [70], Batch [400/484], Loss: 0.4668, LR: 0.001000\n",
      "Evaluation Loss: 0.3618, Accuracy: 85.40%\n",
      "New best model saved with accuracy: 85.40%\n",
      "Epoch [70/300], Train Loss: 0.0833, Acc:  78.27%\n",
      "Epoch [71], Batch [100/484], Loss: 0.4599, LR: 0.001000\n",
      "Epoch [71], Batch [200/484], Loss: 0.4566, LR: 0.001000\n",
      "Epoch [71], Batch [300/484], Loss: 0.4596, LR: 0.001000\n",
      "Epoch [71], Batch [400/484], Loss: 0.4669, LR: 0.001000\n",
      "Evaluation Loss: 0.3666, Accuracy: 85.25%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [72], Batch [100/484], Loss: 0.4474, LR: 0.001000\n",
      "Epoch [72], Batch [200/484], Loss: 0.4550, LR: 0.001000\n",
      "Epoch [72], Batch [300/484], Loss: 0.4697, LR: 0.001000\n",
      "Epoch [72], Batch [400/484], Loss: 0.4692, LR: 0.001000\n",
      "Evaluation Loss: 0.3612, Accuracy: 85.51%\n",
      "New best model saved with accuracy: 85.51%\n",
      "Epoch [73], Batch [100/484], Loss: 0.4487, LR: 0.001000\n",
      "Epoch [73], Batch [200/484], Loss: 0.4674, LR: 0.001000\n",
      "Epoch [73], Batch [300/484], Loss: 0.4692, LR: 0.001000\n",
      "Epoch [73], Batch [400/484], Loss: 0.4580, LR: 0.001000\n",
      "Evaluation Loss: 0.3661, Accuracy: 85.56%\n",
      "New best model saved with accuracy: 85.56%\n",
      "Epoch [74], Batch [100/484], Loss: 0.4543, LR: 0.001000\n",
      "Epoch [74], Batch [200/484], Loss: 0.4558, LR: 0.001000\n",
      "Epoch [74], Batch [300/484], Loss: 0.4513, LR: 0.001000\n",
      "Epoch [74], Batch [400/484], Loss: 0.4779, LR: 0.001000\n",
      "Evaluation Loss: 0.3590, Accuracy: 85.48%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [75], Batch [100/484], Loss: 0.4536, LR: 0.001000\n",
      "Epoch [75], Batch [200/484], Loss: 0.4613, LR: 0.001000\n",
      "Epoch [75], Batch [300/484], Loss: 0.4647, LR: 0.001000\n",
      "Epoch [75], Batch [400/484], Loss: 0.4563, LR: 0.001000\n",
      "Evaluation Loss: 0.3614, Accuracy: 85.43%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [76], Batch [100/484], Loss: 0.4618, LR: 0.001000\n",
      "Epoch [76], Batch [200/484], Loss: 0.4521, LR: 0.001000\n",
      "Epoch [76], Batch [300/484], Loss: 0.4510, LR: 0.001000\n",
      "Epoch [76], Batch [400/484], Loss: 0.4563, LR: 0.001000\n",
      "Evaluation Loss: 0.3578, Accuracy: 85.45%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [77], Batch [100/484], Loss: 0.4572, LR: 0.001000\n",
      "Epoch [77], Batch [200/484], Loss: 0.4601, LR: 0.001000\n",
      "Epoch [77], Batch [300/484], Loss: 0.4661, LR: 0.001000\n",
      "Epoch [77], Batch [400/484], Loss: 0.4574, LR: 0.001000\n",
      "Evaluation Loss: 0.3574, Accuracy: 85.34%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [78], Batch [100/484], Loss: 0.4485, LR: 0.001000\n",
      "Epoch [78], Batch [200/484], Loss: 0.4658, LR: 0.001000\n",
      "Epoch [78], Batch [300/484], Loss: 0.4512, LR: 0.001000\n",
      "Epoch [78], Batch [400/484], Loss: 0.4522, LR: 0.001000\n",
      "Evaluation Loss: 0.3603, Accuracy: 85.60%\n",
      "New best model saved with accuracy: 85.60%\n",
      "Epoch [79], Batch [100/484], Loss: 0.4532, LR: 0.001000\n",
      "Epoch [79], Batch [200/484], Loss: 0.4537, LR: 0.001000\n",
      "Epoch [79], Batch [300/484], Loss: 0.4651, LR: 0.001000\n",
      "Epoch [79], Batch [400/484], Loss: 0.4526, LR: 0.001000\n",
      "Evaluation Loss: 0.3562, Accuracy: 85.68%\n",
      "New best model saved with accuracy: 85.68%\n",
      "Epoch [80], Batch [100/484], Loss: 0.4531, LR: 0.001000\n",
      "Epoch [80], Batch [200/484], Loss: 0.4587, LR: 0.001000\n",
      "Epoch [80], Batch [300/484], Loss: 0.4522, LR: 0.001000\n",
      "Epoch [80], Batch [400/484], Loss: 0.4511, LR: 0.001000\n",
      "Evaluation Loss: 0.3586, Accuracy: 85.82%\n",
      "New best model saved with accuracy: 85.82%\n",
      "Epoch [80/300], Train Loss: 0.0823, Acc:  78.27%\n",
      "Epoch [81], Batch [100/484], Loss: 0.4584, LR: 0.001000\n",
      "Epoch [81], Batch [200/484], Loss: 0.4615, LR: 0.001000\n",
      "Epoch [81], Batch [300/484], Loss: 0.4540, LR: 0.001000\n",
      "Epoch [81], Batch [400/484], Loss: 0.4568, LR: 0.001000\n",
      "Evaluation Loss: 0.3561, Accuracy: 85.58%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [82], Batch [100/484], Loss: 0.4556, LR: 0.001000\n",
      "Epoch [82], Batch [200/484], Loss: 0.4550, LR: 0.001000\n",
      "Epoch [82], Batch [300/484], Loss: 0.4601, LR: 0.001000\n",
      "Epoch [82], Batch [400/484], Loss: 0.4668, LR: 0.001000\n",
      "Evaluation Loss: 0.3578, Accuracy: 85.64%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [83], Batch [100/484], Loss: 0.4566, LR: 0.001000\n",
      "Epoch [83], Batch [200/484], Loss: 0.4463, LR: 0.001000\n",
      "Epoch [83], Batch [300/484], Loss: 0.4689, LR: 0.001000\n",
      "Epoch [83], Batch [400/484], Loss: 0.4584, LR: 0.001000\n",
      "Evaluation Loss: 0.3552, Accuracy: 85.66%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [84], Batch [100/484], Loss: 0.4493, LR: 0.001000\n",
      "Epoch [84], Batch [200/484], Loss: 0.4487, LR: 0.001000\n",
      "Epoch [84], Batch [300/484], Loss: 0.4554, LR: 0.001000\n",
      "Epoch [84], Batch [400/484], Loss: 0.4684, LR: 0.001000\n",
      "Evaluation Loss: 0.3559, Accuracy: 85.62%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [85], Batch [100/484], Loss: 0.4500, LR: 0.001000\n",
      "Epoch [85], Batch [200/484], Loss: 0.4480, LR: 0.001000\n",
      "Epoch [85], Batch [300/484], Loss: 0.4572, LR: 0.001000\n",
      "Epoch [85], Batch [400/484], Loss: 0.4607, LR: 0.001000\n",
      "Evaluation Loss: 0.3544, Accuracy: 85.78%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [86], Batch [100/484], Loss: 0.4465, LR: 0.001000\n",
      "Epoch [86], Batch [200/484], Loss: 0.4447, LR: 0.001000\n",
      "Epoch [86], Batch [300/484], Loss: 0.4591, LR: 0.001000\n",
      "Epoch [86], Batch [400/484], Loss: 0.4569, LR: 0.001000\n",
      "Evaluation Loss: 0.3489, Accuracy: 85.83%\n",
      "New best model saved with accuracy: 85.83%\n",
      "Epoch [87], Batch [100/484], Loss: 0.4449, LR: 0.001000\n",
      "Epoch [87], Batch [200/484], Loss: 0.4409, LR: 0.001000\n",
      "Epoch [87], Batch [300/484], Loss: 0.4573, LR: 0.001000\n",
      "Epoch [87], Batch [400/484], Loss: 0.4699, LR: 0.001000\n",
      "Evaluation Loss: 0.3542, Accuracy: 85.96%\n",
      "New best model saved with accuracy: 85.96%\n",
      "Epoch [88], Batch [100/484], Loss: 0.4546, LR: 0.001000\n",
      "Epoch [88], Batch [200/484], Loss: 0.4503, LR: 0.001000\n",
      "Epoch [88], Batch [300/484], Loss: 0.4436, LR: 0.001000\n",
      "Epoch [88], Batch [400/484], Loss: 0.4562, LR: 0.001000\n",
      "Evaluation Loss: 0.3518, Accuracy: 85.98%\n",
      "New best model saved with accuracy: 85.98%\n",
      "Epoch [89], Batch [100/484], Loss: 0.4427, LR: 0.001000\n",
      "Epoch [89], Batch [200/484], Loss: 0.4466, LR: 0.001000\n",
      "Epoch [89], Batch [300/484], Loss: 0.4469, LR: 0.001000\n",
      "Epoch [89], Batch [400/484], Loss: 0.4486, LR: 0.001000\n",
      "Evaluation Loss: 0.3502, Accuracy: 85.80%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [90], Batch [100/484], Loss: 0.4535, LR: 0.001000\n",
      "Epoch [90], Batch [200/484], Loss: 0.4424, LR: 0.001000\n",
      "Epoch [90], Batch [300/484], Loss: 0.4529, LR: 0.001000\n",
      "Epoch [90], Batch [400/484], Loss: 0.4603, LR: 0.001000\n",
      "Evaluation Loss: 0.3522, Accuracy: 85.90%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [90/300], Train Loss: 0.0789, Acc:  78.27%\n",
      "Epoch [91], Batch [100/484], Loss: 0.4429, LR: 0.001000\n",
      "Epoch [91], Batch [200/484], Loss: 0.4495, LR: 0.001000\n",
      "Epoch [91], Batch [300/484], Loss: 0.4689, LR: 0.001000\n",
      "Epoch [91], Batch [400/484], Loss: 0.4617, LR: 0.001000\n",
      "Evaluation Loss: 0.3506, Accuracy: 85.78%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [92], Batch [100/484], Loss: 0.4594, LR: 0.001000\n",
      "Epoch [92], Batch [200/484], Loss: 0.4510, LR: 0.001000\n",
      "Epoch [92], Batch [300/484], Loss: 0.4555, LR: 0.001000\n",
      "Epoch [92], Batch [400/484], Loss: 0.4505, LR: 0.001000\n",
      "Evaluation Loss: 0.3514, Accuracy: 85.84%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [93], Batch [100/484], Loss: 0.4512, LR: 0.001000\n",
      "Epoch [93], Batch [200/484], Loss: 0.4461, LR: 0.001000\n",
      "Epoch [93], Batch [300/484], Loss: 0.4464, LR: 0.001000\n",
      "Epoch [93], Batch [400/484], Loss: 0.4622, LR: 0.001000\n",
      "Evaluation Loss: 0.3498, Accuracy: 85.83%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [94], Batch [100/484], Loss: 0.4452, LR: 0.001000\n",
      "Epoch [94], Batch [200/484], Loss: 0.4571, LR: 0.001000\n",
      "Epoch [94], Batch [300/484], Loss: 0.4478, LR: 0.001000\n",
      "Epoch [94], Batch [400/484], Loss: 0.4512, LR: 0.001000\n",
      "Evaluation Loss: 0.3491, Accuracy: 85.85%\n",
      "No improvement for 6/10 epochs\n",
      "Epoch [95], Batch [100/484], Loss: 0.4513, LR: 0.000500\n",
      "Epoch [95], Batch [200/484], Loss: 0.4405, LR: 0.000500\n",
      "Epoch [95], Batch [300/484], Loss: 0.4501, LR: 0.000500\n",
      "Epoch [95], Batch [400/484], Loss: 0.4259, LR: 0.000500\n",
      "Evaluation Loss: 0.3412, Accuracy: 86.17%\n",
      "New best model saved with accuracy: 86.17%\n",
      "Epoch [96], Batch [100/484], Loss: 0.4280, LR: 0.000500\n",
      "Epoch [96], Batch [200/484], Loss: 0.4292, LR: 0.000500\n",
      "Epoch [96], Batch [300/484], Loss: 0.4476, LR: 0.000500\n",
      "Epoch [96], Batch [400/484], Loss: 0.4477, LR: 0.000500\n",
      "Evaluation Loss: 0.3387, Accuracy: 86.45%\n",
      "New best model saved with accuracy: 86.45%\n",
      "Epoch [97], Batch [100/484], Loss: 0.4317, LR: 0.000500\n",
      "Epoch [97], Batch [200/484], Loss: 0.4460, LR: 0.000500\n",
      "Epoch [97], Batch [300/484], Loss: 0.4346, LR: 0.000500\n",
      "Epoch [97], Batch [400/484], Loss: 0.4378, LR: 0.000500\n",
      "Evaluation Loss: 0.3397, Accuracy: 86.37%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [98], Batch [100/484], Loss: 0.4425, LR: 0.000500\n",
      "Epoch [98], Batch [200/484], Loss: 0.4345, LR: 0.000500\n",
      "Epoch [98], Batch [300/484], Loss: 0.4442, LR: 0.000500\n",
      "Epoch [98], Batch [400/484], Loss: 0.4367, LR: 0.000500\n",
      "Evaluation Loss: 0.3365, Accuracy: 86.29%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [99], Batch [100/484], Loss: 0.4347, LR: 0.000500\n",
      "Epoch [99], Batch [200/484], Loss: 0.4456, LR: 0.000500\n",
      "Epoch [99], Batch [300/484], Loss: 0.4347, LR: 0.000500\n",
      "Epoch [99], Batch [400/484], Loss: 0.4528, LR: 0.000500\n",
      "Evaluation Loss: 0.3391, Accuracy: 86.21%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [100], Batch [100/484], Loss: 0.4299, LR: 0.000500\n",
      "Epoch [100], Batch [200/484], Loss: 0.4325, LR: 0.000500\n",
      "Epoch [100], Batch [300/484], Loss: 0.4379, LR: 0.000500\n",
      "Epoch [100], Batch [400/484], Loss: 0.4410, LR: 0.000500\n",
      "Evaluation Loss: 0.3345, Accuracy: 86.58%\n",
      "New best model saved with accuracy: 86.58%\n",
      "Epoch [100/300], Train Loss: 0.0755, Acc:  78.27%\n",
      "Epoch [101], Batch [100/484], Loss: 0.4278, LR: 0.000500\n",
      "Epoch [101], Batch [200/484], Loss: 0.4302, LR: 0.000500\n",
      "Epoch [101], Batch [300/484], Loss: 0.4365, LR: 0.000500\n",
      "Epoch [101], Batch [400/484], Loss: 0.4304, LR: 0.000500\n",
      "Evaluation Loss: 0.3338, Accuracy: 86.57%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [102], Batch [100/484], Loss: 0.4357, LR: 0.000500\n",
      "Epoch [102], Batch [200/484], Loss: 0.4296, LR: 0.000500\n",
      "Epoch [102], Batch [300/484], Loss: 0.4475, LR: 0.000500\n",
      "Epoch [102], Batch [400/484], Loss: 0.4380, LR: 0.000500\n",
      "Evaluation Loss: 0.3351, Accuracy: 86.46%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [103], Batch [100/484], Loss: 0.4313, LR: 0.000500\n",
      "Epoch [103], Batch [200/484], Loss: 0.4235, LR: 0.000500\n",
      "Epoch [103], Batch [300/484], Loss: 0.4388, LR: 0.000500\n",
      "Epoch [103], Batch [400/484], Loss: 0.4381, LR: 0.000500\n",
      "Evaluation Loss: 0.3337, Accuracy: 86.57%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [104], Batch [100/484], Loss: 0.4321, LR: 0.000500\n",
      "Epoch [104], Batch [200/484], Loss: 0.4363, LR: 0.000500\n",
      "Epoch [104], Batch [300/484], Loss: 0.4344, LR: 0.000500\n",
      "Epoch [104], Batch [400/484], Loss: 0.4295, LR: 0.000500\n",
      "Evaluation Loss: 0.3339, Accuracy: 86.46%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [105], Batch [100/484], Loss: 0.4265, LR: 0.000500\n",
      "Epoch [105], Batch [200/484], Loss: 0.4308, LR: 0.000500\n",
      "Epoch [105], Batch [300/484], Loss: 0.4274, LR: 0.000500\n",
      "Epoch [105], Batch [400/484], Loss: 0.4454, LR: 0.000500\n",
      "Evaluation Loss: 0.3353, Accuracy: 86.53%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [106], Batch [100/484], Loss: 0.4267, LR: 0.000500\n",
      "Epoch [106], Batch [200/484], Loss: 0.4271, LR: 0.000500\n",
      "Epoch [106], Batch [300/484], Loss: 0.4296, LR: 0.000500\n",
      "Epoch [106], Batch [400/484], Loss: 0.4373, LR: 0.000500\n",
      "Evaluation Loss: 0.3351, Accuracy: 86.41%\n",
      "No improvement for 6/10 epochs\n",
      "Epoch [107], Batch [100/484], Loss: 0.4318, LR: 0.000250\n",
      "Epoch [107], Batch [200/484], Loss: 0.4272, LR: 0.000250\n",
      "Epoch [107], Batch [300/484], Loss: 0.4309, LR: 0.000250\n",
      "Epoch [107], Batch [400/484], Loss: 0.4319, LR: 0.000250\n",
      "Evaluation Loss: 0.3331, Accuracy: 86.53%\n",
      "No improvement for 7/10 epochs\n",
      "Epoch [108], Batch [100/484], Loss: 0.4077, LR: 0.000250\n",
      "Epoch [108], Batch [200/484], Loss: 0.4423, LR: 0.000250\n",
      "Epoch [108], Batch [300/484], Loss: 0.4229, LR: 0.000250\n",
      "Epoch [108], Batch [400/484], Loss: 0.4255, LR: 0.000250\n",
      "Evaluation Loss: 0.3294, Accuracy: 86.55%\n",
      "No improvement for 8/10 epochs\n",
      "Epoch [109], Batch [100/484], Loss: 0.4233, LR: 0.000250\n",
      "Epoch [109], Batch [200/484], Loss: 0.4244, LR: 0.000250\n",
      "Epoch [109], Batch [300/484], Loss: 0.4361, LR: 0.000250\n",
      "Epoch [109], Batch [400/484], Loss: 0.4237, LR: 0.000250\n",
      "Evaluation Loss: 0.3297, Accuracy: 86.64%\n",
      "New best model saved with accuracy: 86.64%\n",
      "Epoch [110], Batch [100/484], Loss: 0.4315, LR: 0.000250\n",
      "Epoch [110], Batch [200/484], Loss: 0.4384, LR: 0.000250\n",
      "Epoch [110], Batch [300/484], Loss: 0.4159, LR: 0.000250\n",
      "Epoch [110], Batch [400/484], Loss: 0.4321, LR: 0.000250\n",
      "Evaluation Loss: 0.3277, Accuracy: 86.77%\n",
      "New best model saved with accuracy: 86.77%\n",
      "Epoch [110/300], Train Loss: 0.0736, Acc:  78.27%\n",
      "Epoch [111], Batch [100/484], Loss: 0.4228, LR: 0.000250\n",
      "Epoch [111], Batch [200/484], Loss: 0.4263, LR: 0.000250\n",
      "Epoch [111], Batch [300/484], Loss: 0.4327, LR: 0.000250\n",
      "Epoch [111], Batch [400/484], Loss: 0.4330, LR: 0.000250\n",
      "Evaluation Loss: 0.3256, Accuracy: 86.88%\n",
      "New best model saved with accuracy: 86.88%\n",
      "Epoch [112], Batch [100/484], Loss: 0.4236, LR: 0.000250\n",
      "Epoch [112], Batch [200/484], Loss: 0.4234, LR: 0.000250\n",
      "Epoch [112], Batch [300/484], Loss: 0.4269, LR: 0.000250\n",
      "Epoch [112], Batch [400/484], Loss: 0.4238, LR: 0.000250\n",
      "Evaluation Loss: 0.3271, Accuracy: 86.73%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [113], Batch [100/484], Loss: 0.4127, LR: 0.000250\n",
      "Epoch [113], Batch [200/484], Loss: 0.4264, LR: 0.000250\n",
      "Epoch [113], Batch [300/484], Loss: 0.4250, LR: 0.000250\n",
      "Epoch [113], Batch [400/484], Loss: 0.4228, LR: 0.000250\n",
      "Evaluation Loss: 0.3296, Accuracy: 86.68%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [114], Batch [100/484], Loss: 0.4290, LR: 0.000250\n",
      "Epoch [114], Batch [200/484], Loss: 0.4377, LR: 0.000250\n",
      "Epoch [114], Batch [300/484], Loss: 0.4235, LR: 0.000250\n",
      "Epoch [114], Batch [400/484], Loss: 0.4152, LR: 0.000250\n",
      "Evaluation Loss: 0.3258, Accuracy: 86.77%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [115], Batch [100/484], Loss: 0.4162, LR: 0.000250\n",
      "Epoch [115], Batch [200/484], Loss: 0.4323, LR: 0.000250\n",
      "Epoch [115], Batch [300/484], Loss: 0.4248, LR: 0.000250\n",
      "Epoch [115], Batch [400/484], Loss: 0.4244, LR: 0.000250\n",
      "Evaluation Loss: 0.3237, Accuracy: 86.85%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [116], Batch [100/484], Loss: 0.4220, LR: 0.000250\n",
      "Epoch [116], Batch [200/484], Loss: 0.4251, LR: 0.000250\n",
      "Epoch [116], Batch [300/484], Loss: 0.4207, LR: 0.000250\n",
      "Epoch [116], Batch [400/484], Loss: 0.4315, LR: 0.000250\n",
      "Evaluation Loss: 0.3237, Accuracy: 86.96%\n",
      "New best model saved with accuracy: 86.96%\n",
      "Epoch [117], Batch [100/484], Loss: 0.4187, LR: 0.000250\n",
      "Epoch [117], Batch [200/484], Loss: 0.4237, LR: 0.000250\n",
      "Epoch [117], Batch [300/484], Loss: 0.4258, LR: 0.000250\n",
      "Epoch [117], Batch [400/484], Loss: 0.4281, LR: 0.000250\n",
      "Evaluation Loss: 0.3248, Accuracy: 87.01%\n",
      "New best model saved with accuracy: 87.01%\n",
      "Epoch [118], Batch [100/484], Loss: 0.4133, LR: 0.000250\n",
      "Epoch [118], Batch [200/484], Loss: 0.4324, LR: 0.000250\n",
      "Epoch [118], Batch [300/484], Loss: 0.4198, LR: 0.000250\n",
      "Epoch [118], Batch [400/484], Loss: 0.4292, LR: 0.000250\n",
      "Evaluation Loss: 0.3253, Accuracy: 86.81%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [119], Batch [100/484], Loss: 0.4334, LR: 0.000250\n",
      "Epoch [119], Batch [200/484], Loss: 0.4194, LR: 0.000250\n",
      "Epoch [119], Batch [300/484], Loss: 0.4237, LR: 0.000250\n",
      "Epoch [119], Batch [400/484], Loss: 0.4214, LR: 0.000250\n",
      "Evaluation Loss: 0.3226, Accuracy: 86.88%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [120], Batch [100/484], Loss: 0.4317, LR: 0.000250\n",
      "Epoch [120], Batch [200/484], Loss: 0.4242, LR: 0.000250\n",
      "Epoch [120], Batch [300/484], Loss: 0.4291, LR: 0.000250\n",
      "Epoch [120], Batch [400/484], Loss: 0.4145, LR: 0.000250\n",
      "Evaluation Loss: 0.3237, Accuracy: 86.93%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [120/300], Train Loss: 0.0739, Acc:  78.27%\n",
      "Epoch [121], Batch [100/484], Loss: 0.4292, LR: 0.000250\n",
      "Epoch [121], Batch [200/484], Loss: 0.4118, LR: 0.000250\n",
      "Epoch [121], Batch [300/484], Loss: 0.4126, LR: 0.000250\n",
      "Epoch [121], Batch [400/484], Loss: 0.4258, LR: 0.000250\n",
      "Evaluation Loss: 0.3252, Accuracy: 87.00%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [122], Batch [100/484], Loss: 0.4205, LR: 0.000250\n",
      "Epoch [122], Batch [200/484], Loss: 0.4068, LR: 0.000250\n",
      "Epoch [122], Batch [300/484], Loss: 0.4155, LR: 0.000250\n",
      "Epoch [122], Batch [400/484], Loss: 0.4312, LR: 0.000250\n",
      "Evaluation Loss: 0.3239, Accuracy: 86.86%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [123], Batch [100/484], Loss: 0.4237, LR: 0.000250\n",
      "Epoch [123], Batch [200/484], Loss: 0.4095, LR: 0.000250\n",
      "Epoch [123], Batch [300/484], Loss: 0.4259, LR: 0.000250\n",
      "Epoch [123], Batch [400/484], Loss: 0.4312, LR: 0.000250\n",
      "Evaluation Loss: 0.3224, Accuracy: 86.98%\n",
      "No improvement for 6/10 epochs\n",
      "Epoch [124], Batch [100/484], Loss: 0.4122, LR: 0.000125\n",
      "Epoch [124], Batch [200/484], Loss: 0.4263, LR: 0.000125\n",
      "Epoch [124], Batch [300/484], Loss: 0.4147, LR: 0.000125\n",
      "Epoch [124], Batch [400/484], Loss: 0.4315, LR: 0.000125\n",
      "Evaluation Loss: 0.3223, Accuracy: 86.94%\n",
      "No improvement for 7/10 epochs\n",
      "Epoch [125], Batch [100/484], Loss: 0.4227, LR: 0.000125\n",
      "Epoch [125], Batch [200/484], Loss: 0.4104, LR: 0.000125\n",
      "Epoch [125], Batch [300/484], Loss: 0.4275, LR: 0.000125\n",
      "Epoch [125], Batch [400/484], Loss: 0.4246, LR: 0.000125\n",
      "Evaluation Loss: 0.3222, Accuracy: 86.93%\n",
      "No improvement for 8/10 epochs\n",
      "Epoch [126], Batch [100/484], Loss: 0.4143, LR: 0.000125\n",
      "Epoch [126], Batch [200/484], Loss: 0.4212, LR: 0.000125\n",
      "Epoch [126], Batch [300/484], Loss: 0.4220, LR: 0.000125\n",
      "Epoch [126], Batch [400/484], Loss: 0.4228, LR: 0.000125\n",
      "Evaluation Loss: 0.3156, Accuracy: 87.17%\n",
      "New best model saved with accuracy: 87.17%\n",
      "Epoch [127], Batch [100/484], Loss: 0.4075, LR: 0.000125\n",
      "Epoch [127], Batch [200/484], Loss: 0.4164, LR: 0.000125\n",
      "Epoch [127], Batch [300/484], Loss: 0.4254, LR: 0.000125\n",
      "Epoch [127], Batch [400/484], Loss: 0.4338, LR: 0.000125\n",
      "Evaluation Loss: 0.3220, Accuracy: 86.95%\n",
      "No improvement for 1/10 epochs\n",
      "Epoch [128], Batch [100/484], Loss: 0.4220, LR: 0.000125\n",
      "Epoch [128], Batch [200/484], Loss: 0.4218, LR: 0.000125\n",
      "Epoch [128], Batch [300/484], Loss: 0.4171, LR: 0.000125\n",
      "Epoch [128], Batch [400/484], Loss: 0.4081, LR: 0.000125\n",
      "Evaluation Loss: 0.3187, Accuracy: 87.09%\n",
      "No improvement for 2/10 epochs\n",
      "Epoch [129], Batch [100/484], Loss: 0.4213, LR: 0.000125\n",
      "Epoch [129], Batch [200/484], Loss: 0.4142, LR: 0.000125\n",
      "Epoch [129], Batch [300/484], Loss: 0.4283, LR: 0.000125\n",
      "Epoch [129], Batch [400/484], Loss: 0.4147, LR: 0.000125\n",
      "Evaluation Loss: 0.3195, Accuracy: 87.11%\n",
      "No improvement for 3/10 epochs\n",
      "Epoch [130], Batch [100/484], Loss: 0.4198, LR: 0.000125\n",
      "Epoch [130], Batch [200/484], Loss: 0.4197, LR: 0.000125\n",
      "Epoch [130], Batch [300/484], Loss: 0.4271, LR: 0.000125\n",
      "Epoch [130], Batch [400/484], Loss: 0.4186, LR: 0.000125\n",
      "Evaluation Loss: 0.3200, Accuracy: 87.04%\n",
      "No improvement for 4/10 epochs\n",
      "Epoch [130/300], Train Loss: 0.0741, Acc:  78.27%\n",
      "Epoch [131], Batch [100/484], Loss: 0.4146, LR: 0.000125\n",
      "Epoch [131], Batch [200/484], Loss: 0.4224, LR: 0.000125\n",
      "Epoch [131], Batch [300/484], Loss: 0.4194, LR: 0.000125\n",
      "Epoch [131], Batch [400/484], Loss: 0.4165, LR: 0.000125\n",
      "Evaluation Loss: 0.3189, Accuracy: 87.15%\n",
      "No improvement for 5/10 epochs\n",
      "Epoch [132], Batch [100/484], Loss: 0.4174, LR: 0.000125\n",
      "Epoch [132], Batch [200/484], Loss: 0.4220, LR: 0.000125\n",
      "Epoch [132], Batch [300/484], Loss: 0.4237, LR: 0.000125\n",
      "Epoch [132], Batch [400/484], Loss: 0.4220, LR: 0.000125\n",
      "Evaluation Loss: 0.3218, Accuracy: 87.06%\n",
      "No improvement for 6/10 epochs\n",
      "Epoch [133], Batch [100/484], Loss: 0.4050, LR: 0.000063\n",
      "Epoch [133], Batch [200/484], Loss: 0.4208, LR: 0.000063\n",
      "Epoch [133], Batch [300/484], Loss: 0.4215, LR: 0.000063\n",
      "Epoch [133], Batch [400/484], Loss: 0.4141, LR: 0.000063\n",
      "Evaluation Loss: 0.3209, Accuracy: 87.07%\n",
      "No improvement for 7/10 epochs\n",
      "Epoch [134], Batch [100/484], Loss: 0.4200, LR: 0.000063\n",
      "Epoch [134], Batch [200/484], Loss: 0.4159, LR: 0.000063\n",
      "Epoch [134], Batch [300/484], Loss: 0.4260, LR: 0.000063\n",
      "Epoch [134], Batch [400/484], Loss: 0.4224, LR: 0.000063\n",
      "Evaluation Loss: 0.3194, Accuracy: 87.05%\n",
      "No improvement for 8/10 epochs\n",
      "Epoch [135], Batch [100/484], Loss: 0.4146, LR: 0.000063\n",
      "Epoch [135], Batch [200/484], Loss: 0.4226, LR: 0.000063\n",
      "Epoch [135], Batch [300/484], Loss: 0.4200, LR: 0.000063\n",
      "Epoch [135], Batch [400/484], Loss: 0.4207, LR: 0.000063\n",
      "Evaluation Loss: 0.3177, Accuracy: 87.00%\n",
      "No improvement for 9/10 epochs\n",
      "Epoch [136], Batch [100/484], Loss: 0.4241, LR: 0.000063\n",
      "Epoch [136], Batch [200/484], Loss: 0.4146, LR: 0.000063\n",
      "Epoch [136], Batch [300/484], Loss: 0.4169, LR: 0.000063\n",
      "Epoch [136], Batch [400/484], Loss: 0.4021, LR: 0.000063\n",
      "Evaluation Loss: 0.3183, Accuracy: 87.13%\n",
      "No improvement for 10/10 epochs\n",
      "Early stopping at epoch 136\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:\n",
    "            avg_loss = running_loss / 100\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch [{epoch+1}], Batch [{batch_idx+1}/{total_batches}], Loss: {avg_loss:.4f}, LR: {current_lr:.6f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return running_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate(): # \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eval_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = eval_loss / len(train_loader)\n",
    "    print(f'Evaluation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    epoch_loss = train(epoch)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    accuracy, eval_loss = evaluate()\n",
    "    val_accuracies.append(train_accuracy)\n",
    "\n",
    "    scheduler.step(accuracy)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'New best model saved with accuracy: {best_accuracy:.2f}%')\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f'No improvement for {no_improve_count}/{patience} epochs')\n",
    "\n",
    "        if no_improve_count >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCH}], Train Loss: {epoch_loss:.4f}, Acc: {train_accuracy: .2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "92e234d4-7ca6-4156-89a8-0fa28de4b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "test_id = test_set.id.tolist() if hasattr(test_set, 'id') else []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.append(predicted)\n",
    "\n",
    "all_predictions = torch.cat(all_predictions, dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "69e68e67-9585-456e-a240-5c5b95cf6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_columns = [f'Class_{i}' for i in range(1, 10)]\n",
    "one_hot_predictions = np.zeros((len(all_predictions), len(class_columns)))\n",
    "one_hot_predictions[np.arange(len(all_predictions)), all_predictions] = 1\n",
    "\n",
    "submission = pd.DataFrame(one_hot_predictions, columns=class_columns)\n",
    "submission.insert(0, 'id', test_id[:len(all_predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8932b595-ba4d-4809-915a-a618e0a1cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv('otto_submission.csv', index=False)\n",
    "print('Submission file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b472bc2-084d-4c11-ad90-6062cd2a207b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
